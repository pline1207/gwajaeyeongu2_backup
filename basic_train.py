import os
import argparse
import numpy as np
from tqdm import tqdm
from contextlib import ExitStack
import nibabel as nib
import json

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
import pandas as pd 

# spharmnet ì„í¬íŠ¸
from spharmnet import SPHARM_Net, SphericalTrainer, GaussianDiffusion
from spharmnet.lib.io import read_mesh, read_dat


class OASISLongitudinalDataset(Dataset):
    def __init__(self, csv_path, surf_base_dir, in_ch, hemi, 
                 subject_to_id_map: dict,
                 subject_partition_set: set,
                 max_time_obs_overall: float, # ì—¬ê¸°ì„œëŠ” max_ageë¡œ ì“°ê±°ë‚˜ ë¬´ì‹œ ê°€ëŠ¥
                 data_norm=False, preload="none", data_normalization=False, 
                 sphere_path=None, 
                 partition='train'
                 ):
        super().__init__()
        
        self.surf_base_dir = os.path.join(surf_base_dir, "features") 
        self.in_ch = in_ch
        self.hemi = hemi
        self.preload = preload
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.partition = partition

        if sphere_path is None:
            raise ValueError("OASISLongitudinalDataset requires 'sphere_path'.")
        
        ico_v, _ = read_mesh(sphere_path) 
        self.num_vertices = ico_v.shape[0]

        # 1. Excel ë¡œë“œ
        full_df = pd.read_excel(csv_path) 
        full_df = full_df.dropna(subset=['Subject ID'])
        
        # Age ì»¬ëŸ¼ í•„ìˆ˜ í™•ì¸
        if 'Age' not in full_df.columns:
            raise ValueError("Excel íŒŒì¼ì— 'Age' ì»¬ëŸ¼ì´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.")

        # Partitioning
        self.df = full_df[full_df['Subject ID'].isin(subject_partition_set)].reset_index(drop=True)
        
        self.subject_to_id = subject_to_id_map
        self.num_subjects = len(subject_to_id_map) 
        self.max_time_obs = max_time_obs_overall # ì‚¬ìš© ì•ˆ í•  ìˆ˜ë„ ìˆìŒ (Age/100 ì‚¬ìš©ì‹œ)

        print(f"[{partition.upper()}] Loaded {len(self.df)} scans.")

    def __len__(self):
        return len(self.df)

    # ğŸŒŸ [Helper í•¨ìˆ˜] íŒŒì¼ ë¡œë“œ ë° ì •ê·œí™” ë¡œì§ ë¶„ë¦¬ (ì¤‘ë³µ ì œê±°)
    def _load_scan_data(self, mri_id):
        all_channels_data = []
        for h in self.hemi:    
            for ch in self.in_ch: 
                file_name = f"{mri_id}.{h}.aug0.{ch}.dat"
                file_path = os.path.join(self.surf_base_dir, file_name)
                
                try:
                    surface_data = read_dat(file_path, self.num_vertices)
                    
                    # ì •ê·œí™”
                    if 'thickness' in ch:
                        surface_data = np.clip(surface_data, 0, 5.0) / 5.0
                    elif 'curv' in ch or 'sulc' in ch or 'K1' in ch or 'K2' in ch:
                        surface_data = (np.clip(surface_data, -1.0, 1.0) + 1.0) / 2.0
                    else:
                        surface_data = np.clip(surface_data, 0.0, 1.0)

                    if surface_data.ndim == 1:
                        surface_data = surface_data[np.newaxis, :] 
                        
                    all_channels_data.append(surface_data)
                    
                except Exception as e:
                    print(f"Error loading: {file_path}") 
                    # ì—ëŸ¬ ë°œìƒ ì‹œ 0ìœ¼ë¡œ ì±„ìš´ ë”ë¯¸ ë°ì´í„° ë°˜í™˜ (í•™ìŠµì´ ì£½ì§€ ì•Šë„ë¡) í˜¹ì€ raise
                    raise e

        data = np.concatenate(all_channels_data, axis=0) 
        return data.astype(np.float32)

    def __getitem__(self, idx):
        # 1. í˜„ì¬(Target) ìƒ˜í”Œ ì •ë³´
        row = self.df.iloc[idx]
        subject_id_str = row['Subject ID']
        mri_id = row['MRI ID']
        
        # ğŸŒŸ Age ì •ê·œí™” (0~1 ë²”ìœ„ë¡œ ë§ì¶¤, ì˜ˆë¥¼ ë“¤ì–´ 100ì„¸ ê¸°ì¤€)
        age = float(row['Age']) / 100.0 
        
        subject_id_int = int(self.subject_to_id[subject_id_str])
        
        # 2. Target ë°ì´í„° ë¡œë“œ
        target_data = self._load_scan_data(mri_id)

        # 3. ğŸŒŸ [í•µì‹¬] Reference ë°ì´í„° ì°¾ê¸° (ê°™ì€ ì‚¬ëŒì˜ ë‹¤ë¥¸ ë°ì´í„°)
        # í˜„ì¬ ë°ì´í„°í”„ë ˆì„ì—ì„œ ê°™ì€ IDë¥¼ ê°€ì§„ í–‰ë“¤ì„ ì°¾ìŒ
        subj_rows = self.df[self.df['Subject ID'] == subject_id_str]
        
        # í˜„ì¬ ìŠ¤ìº”(idx)ì„ ì œì™¸í•œ ë‚˜ë¨¸ì§€ í›„ë³´êµ°
        # (ì£¼ì˜: iloc indexì™€ df indexê°€ ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ MRI IDë¡œ ë¹„êµ ì¶”ì²œ)
        candidates = subj_rows[subj_rows['MRI ID'] != mri_id]
        
        if len(candidates) > 0:
            # ë‹¤ë¥¸ ì‹œì ì˜ ë°ì´í„°ê°€ ìˆë‹¤ë©´ ëœë¤í•˜ê²Œ í•˜ë‚˜ ì„ íƒ (Data Augmentation íš¨ê³¼)
            ref_row = candidates.sample(n=1).iloc[0]
            ref_mri_id = ref_row['MRI ID']
            ref_data = self._load_scan_data(ref_mri_id)
        else:
            # ë§Œì•½ ìŠ¤ìº”ì´ í•˜ë‚˜ë¿ì¸ ì‚¬ëŒì´ë¼ë©´? ìê¸° ìì‹ ì„ Referenceë¡œ ì‚¬ìš© (í˜¹ì€ Zero tensor)
            # ì—¬ê¸°ì„œëŠ” ìê¸° ìì‹ ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì½”ë“œ ì•ˆì •ì„±ì— ì¢‹ìŒ
            ref_data = target_data.copy()

        # 4. ë¦¬í„´: (Target, MRI_ID, Age, ID, Reference)
        # new_model.pyì˜ collate_fn ìˆœì„œì— ë§ì¶¤: data, name, time, id, ref
        return target_data, mri_id, age, subject_id_int, ref_data


def get_args():
    parser = argparse.ArgumentParser()

    # Dataset & dataloader
    parser.add_argument("--sphere", type=str, default="/data/object/sphere/unist/icosphere_6.vtk", help="Sphere mesh (vtk or FreeSurfer format)")
    parser.add_argument("--data-norm", action="store_true", help="Z-score+prctile data normalization")
    parser.add_argument("--preload", type=str, choices=["none", "cpu", "device"], default="device", help="Data preloading")
    parser.add_argument("--in-ch", type=str, default=["curv", "sulc", "inflated.H"], nargs="+", help="List of geometry")
    parser.add_argument("--hemi", type=str, nargs="+", choices=["lh", "rh"], help="Hemisphere for learning", required=True)
    
    # [ìˆ˜ì •] data-dir ëŒ€ì‹  csv/surf-dir ì‚¬ìš©
    parser.add_argument("--csv-file", type=str, default="/data/human/OASIS/OASIS2/oasis_longitudinal_demographics.xlsx", required=True, help="OASIS longitudinal Excel íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--surf-dir", type=str, default="/data/human/OASIS/OASIS2/Freesurfer", required=True, help="Freesurfer í‘œë©´ ë°ì´í„°ê°€ ìˆëŠ” ê¸°ë³¸ ë””ë ‰í† ë¦¬")
    
    parser.add_argument("--classes", type=int, nargs="+", help="List of regions of interest")
    parser.add_argument("--seed", type=int, default=0, help="Random seed for data shuffling")
    parser.add_argument("--aug", type=int, default=0, help="Level of data augmentation")


    parser.add_argument("--test-split-ratio", type=float, default=0.2,help="í…ŒìŠ¤íŠ¸ì…‹ìœ¼ë¡œ ë¶„ë¦¬í•  í”¼í—˜ì ë¹„ìœ¨ (e.g., 0.2 = 20%)")
    
    # (ì´í•˜ ë‚˜ë¨¸ì§€ argsëŠ” ë™ì¼)
    # Training and evaluation
    parser.add_argument("--train-num-steps", type=int, default=20, help="Max epoch")
    parser.add_argument("--batch-size", type=int, default=1, help="Batch size")
    parser.add_argument("--learning-rate", type=float, default=8e-5, help="Initial learning rate")
    parser.add_argument("--no-decay", action="store_true", help="Disable decay (every 2 epochs if no progress)")
    parser.add_argument("--loss", type=str, default="dl", choices=["dl", "ce"], help="dl: Dice loss, ce: cross entropy")
    parser.add_argument("--log-dir", type=str, default="./logs", help="Path to the log files (output)")
    parser.add_argument("--ckpt-dir", type=str, default="./logs", help="Path to the checkpoint file (output)")
    parser.add_argument("--resume", type=str, default=None, help="Checkpoint (pth) to resume training")
    parser.add_argument("--save-and-sample-every", type=int, default=1000)
    parser.add_argument("--num-samples", type=int, default=2)
    parser.add_argument("--results-dir", type=str, default="/data/lfs/pline1207/SPHARM-Diffusion/results")
    parser.add_argument("--data-normalization", action="store_true")
    # diffusion settings
    parser.add_argument("--timesteps", type=int, default=1000)
    parser.add_argument("--sampling-timesteps", type=int, default=None)
    parser.add_argument("--objective", type=str, choices=["pred_v", "pred_x0", "pred_noise"], default="pred_v")
    parser.add_argument("--beta-schedule", type=str, choices=["linear", "cosine", "sigmoid"], default="cosine")
    parser.add_argument("--auto-normalize", action="store_true")
    # SPHARM-Net settings
    parser.add_argument("-D", "--depth", type=int, default=3, help="Depth of SPHARM-Net")
    parser.add_argument("-C", "--channel", type=int, default=128, help="# of channels in the entry layer of SPHARM-Net")
    parser.add_argument("-L", "--bandwidth", type=int, default=80, help="Bandwidth of SPHARM-Net")
    parser.add_argument("--interval", type=int, default=5, help="Anchor interval of hamonic coefficients")
    # Machine settings
    parser.add_argument("--gpu", type=int, default=0, help="GPU ID for training (normally, starting with 0)")
    parser.add_argument("--no-cuda", action="store_true", help="No CUDA")
    parser.add_argument("--threads", type=int, default=1, help="# of CPU threads for basis reconstruction")

    args = parser.parse_args()
    return args


def main(args):
    args.cuda = not args.no_cuda and torch.cuda.is_available()
    device = torch.device(f"cuda:{args.gpu}" if args.cuda else "cpu")
    preload = None if args.preload == "none" else device if args.preload == "device" else args.preload

    torch.set_num_threads(args.threads)
    if not args.cuda:
        torch.set_num_threads(args.threads)

    torch.manual_seed(args.seed)
    if args.cuda:
        torch.cuda.manual_seed(args.seed)

    print("Loading data...")
    sphere = os.path.join(args.sphere)
    v, _ = read_mesh(sphere)

    # ğŸŒŸ --- [ìˆ˜ì •] í†µí•© ID ë§µ  --- ğŸŒŸ
    print(f"ì‚¬ì „ ë¡œë“œ (í†µí•© ID ë§µ ìƒì„±ìš©): {args.csv_file}")
    try:
        # 1. ì „ì²´ ë°ì´í„°(full_df)ë¥¼ ë¨¼ì € ë¡œë“œ
        full_df = pd.read_excel(args.csv_file)
        full_df = full_df.dropna(subset=['Subject ID'])
    except Exception as e:
        raise FileNotFoundError(f"Excel íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {args.csv_file} - {e}")

    # 2. *ì „ì²´* í”¼í—˜ì ê¸°ì¤€ì˜ í†µí•© ID ë§µ ìƒì„±
    all_subjects_sorted = sorted(full_df['Subject ID'].unique())
    total_unique_subjects = len(all_subjects_sorted)
    
    # 3. ì´ê²ƒì´ ìœ ì¼í•œ ID ë§µì´ ë˜ì–´ì•¼ í•¨
    subject_to_id_map = {subj: i for i, subj in enumerate(all_subjects_sorted)}
    
    print(f"ì´ {total_unique_subjects}ëª…ì˜ ê³ ìœ  í”¼í—˜ìì— ëŒ€í•´ í†µí•© ID ë§µ ìƒì„± ì™„ë£Œ.")

    map_file_path = 'subject_to_id_map.json'
    print(f"Subject ID ë§µì„ '{map_file_path}' íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤...")
    with open(map_file_path, 'w', encoding='utf-8') as f:
        json.dump(subject_to_id_map, f, ensure_ascii=False, indent=4)
    print("ì €ì¥ ì™„ë£Œ.")


    # 4. *ì „ì²´* í”¼í—˜ì ê¸°ì¤€ìœ¼ë¡œ í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë¶„ë¦¬ (ë¬¸ìì—´ ê¸°ì¤€)
    rng = np.random.RandomState(args.seed)
    shuffled_subjects = list(all_subjects_sorted) # ë³µì‚¬ë³¸ ìƒì„±
    rng.shuffle(shuffled_subjects)
    
    split_idx = int(total_unique_subjects * (1.0 - args.test_split_ratio))
    train_subjects_set = set(shuffled_subjects[:split_idx])
    test_subjects_set = set(shuffled_subjects[split_idx:])
    
    # 5. *ì „ì²´* ë°ì´í„° ê¸°ì¤€ max_time_obs ê³„ì‚°
    full_df['MR Delay'] = full_df['MR Delay'].apply(lambda x: str(x).replace('M', '').strip())
    full_df['MR Delay'] = pd.to_numeric(full_df['MR Delay'], errors='coerce').fillna(0)
    max_time_obs = full_df['MR Delay'].max()
    # --- [ìˆ˜ì • ì™„ë£Œ] ---


    # [ìˆ˜ì •] í›ˆë ¨ ë°ì´í„°ì…‹(ds_train) ìƒì„±
    # Dataset í´ë˜ìŠ¤ í˜¸ì¶œ ìˆœì„œëŠ” __init__ ì •ì˜ ìˆœì„œì™€ ì¼ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤.
    ds_train = OASISLongitudinalDataset(
        csv_path=args.csv_file,
        surf_base_dir=args.surf_dir,
        in_ch=args.in_ch,
        hemi=args.hemi,
        # --- ğŸŒŸ [ìˆ˜ì •] ìˆœì„œ ë³€ê²½ ---
        subject_to_id_map=subject_to_id_map,      # ğŸ‘ˆ í†µí•© ë§µ ì „ë‹¬
        subject_partition_set=train_subjects_set, # ğŸ‘ˆ í›ˆë ¨ìš© í”¼í—˜ì ëª©ë¡ ì „ë‹¬
        max_time_obs_overall=max_time_obs,        # ğŸ‘ˆ ì „ì²´ ìµœëŒ€ ì‹œê°„ ì „ë‹¬
        # --- ğŸŒŸ ê¸°ë³¸ê°’ ì¸ì ---
        data_norm=args.data_norm,
        preload=preload,
        data_normalization=args.data_normalization,
        sphere_path=sphere,
        partition='train'
        # -----------------
    )

    # ğŸŒŸ [ì¶”ê°€] í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹(ds_test) ìƒì„±
    ds_test = OASISLongitudinalDataset(
        csv_path=args.csv_file,
        surf_base_dir=args.surf_dir,
        in_ch=args.in_ch,
        hemi=args.hemi,
        # --- ğŸŒŸ [ìˆ˜ì •] ìˆœì„œ ë³€ê²½ ---
        subject_to_id_map=subject_to_id_map,      # ğŸ‘ˆ ë™ì¼í•œ í†µí•© ë§µ ì „ë‹¬
        subject_partition_set=test_subjects_set,  # ğŸ‘ˆ í…ŒìŠ¤íŠ¸ìš© í”¼í—˜ì ëª©ë¡ ì „ë‹¬
        max_time_obs_overall=max_time_obs,        # ğŸ‘ˆ ì „ì²´ ìµœëŒ€ ì‹œê°„ ì „ë‹¬
        # --- ğŸŒŸ ê¸°ë³¸ê°’ ì¸ì ---
        data_norm=args.data_norm,
        preload="none", 
        data_normalization=args.data_normalization,
        sphere_path=sphere,
        partition='test'
        # -----------------
    )


    # ğŸŒŸ [ìˆ˜ì •] model_in_channels ë³€ìˆ˜ ì •ì˜
    model_in_channels = len(args.in_ch) * len(args.hemi)
    if model_in_channels == 0:
        raise ValueError("ì…ë ¥ ì±„ë„(--in-ch)ê³¼ ë°˜êµ¬(--hemi)ë¥¼ 1ê°œ ì´ìƒ ì§€ì •í•´ì•¼ í•©ë‹ˆë‹¤.")

    print(f"ëª¨ë¸ì— ì „ë‹¬í•  ì´ ê³ ìœ  í”¼í—˜ì ìˆ˜: {total_unique_subjects}")
    
    # ğŸŒŸ [ìˆ˜ì •] ìˆ˜ì •ëœ SPHARM_Net í˜¸ì¶œ
    model = SPHARM_Net(
        sphere=sphere,
        device=device,
        in_ch=model_in_channels,    # (ìˆ˜ì •)
        n_class=model_in_channels,  # (ìˆ˜ì •) - ì¶œë ¥ë„ ë™ì¼í•˜ë‹¤ê³  ê°€ì •
        C=args.channel,
        L=args.bandwidth,
        D=args.depth,
        interval=args.interval,
        self_condition=False,
        verbose=False,
        add_xyz=True,
        # --- (ì‹ ê·œ) ì¢…ë‹¨ì  ëª¨ë¸ì„ ìœ„í•œ ì¸ì ì „ë‹¬ ---
        max_time_obs=max_time_obs,           # ğŸ‘ˆ ìˆ˜ì •ëœ ë³€ìˆ˜ ì‚¬ìš©
        num_subjects=total_unique_subjects   # ğŸ‘ˆ ìˆ˜ì •ëœ ë³€ìˆ˜ ì‚¬ìš©
        # -------------------------------------
    )
    model.to(device)

    print("train: auto_normalize: ", args.auto_normalize)
    print("train: data_normalization", args.data_normalization)
    diffusion = GaussianDiffusion(
        model=model,
        signal_size=v.shape[0],
        timesteps=args.timesteps,
        sampling_timesteps=args.sampling_timesteps,
        objective=args.objective,
        beta_schedule=args.beta_schedule,
        auto_normalize=args.auto_normalize
    )
    diffusion.to(device)
    

    trainer = SphericalTrainer(
        diffusion_model=diffusion,
        dataset=ds_train, # ğŸŒŸ ds_train ê°ì²´ë¥¼ ì „ë‹¬
        train_batch_size=args.batch_size,
        gradient_accumulate_every=2,
        train_lr=args.learning_rate,
        train_num_steps=args.train_num_steps,
        ema_decay=0.995,
        results_folder=args.results_dir,
        save_and_sample_every=args.save_and_sample_every,
        num_samples=args.num_samples,
        log_dir=args.log_dir,
        valid_dataset=ds_test # ğŸŒŸ ds_test ê°ì²´ë¥¼ ì „ë‹¬
    )
    trainer.train()


if __name__ == "__main__":
    args = get_args()
    main(args)
