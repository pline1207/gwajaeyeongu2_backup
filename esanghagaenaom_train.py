import os
import argparse
import numpy as np
from tqdm import tqdm
from contextlib import ExitStack
import nibabel as nib
import json

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
import pandas as pd 

# spharmnet ì„í¬íŠ¸
from spharmnet import SPHARM_Net, SphericalTrainer, GaussianDiffusion
from spharmnet.lib.io import read_mesh, read_dat


class OASISLongitudinalDataset(Dataset):
    def __init__(self, csv_path, surf_base_dir, in_ch, hemi, 
                 subject_to_id_map: dict,
                 subject_partition_set: set,
                 max_time_obs_overall: float, 
                 data_norm=False, preload="none", data_normalization=False, 
                 sphere_path=None, 
                 partition='train'
                 ):
        super().__init__()
        
        self.surf_base_dir = os.path.join(surf_base_dir, "features") 
        self.in_ch = in_ch
        self.hemi = hemi
        self.preload = preload
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.partition = partition

        if sphere_path is None:
            raise ValueError("OASISLongitudinalDataset requires 'sphere_path'.")
        
        ico_v, _ = read_mesh(sphere_path) 
        self.num_vertices = ico_v.shape[0]

        # 1. Excel ë¡œë“œ
        full_df = pd.read_excel(csv_path) 
        full_df = full_df.dropna(subset=['Subject ID'])
        
        # Age ì»¬ëŸ¼ í•„ìˆ˜ í™•ì¸
        if 'Age' not in full_df.columns:
            raise ValueError("Excel íŒŒì¼ì— 'Age' ì»¬ëŸ¼ì´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.")

        # Partitioning
        self.df = full_df[full_df['Subject ID'].isin(subject_partition_set)].reset_index(drop=True)
        
        self.subject_to_id = subject_to_id_map
        self.num_subjects = len(subject_to_id_map) 
        self.max_time_obs = max_time_obs_overall 

        print(f"[{partition.upper()}] Loaded {len(self.df)} scans.")

    def __len__(self):
        return len(self.df)

    def _load_scan_data(self, mri_id):
        all_channels_data = []
        for h in self.hemi:    
            for ch in self.in_ch: 
                file_name = f"{mri_id}.{h}.aug0.{ch}.dat"
                file_path = os.path.join(self.surf_base_dir, file_name)
                
                try:
                    surface_data = read_dat(file_path, self.num_vertices)
                    
                    # ì •ê·œí™”
                    if 'thickness' in ch:
                        surface_data = np.clip(surface_data, 0, 5.0) / 5.0
                    elif 'curv' in ch or 'sulc' in ch or 'K1' in ch or 'K2' in ch:
                        surface_data = (np.clip(surface_data, -1.0, 1.0) + 1.0) / 2.0
                    else:
                        surface_data = np.clip(surface_data, 0.0, 1.0)

                    if surface_data.ndim == 1:
                        surface_data = surface_data[np.newaxis, :] 
                        
                    all_channels_data.append(surface_data)
                    
                except Exception as e:
                    print(f"Error loading: {file_path}") 
                    raise e

        data = np.concatenate(all_channels_data, axis=0) 
        return data.astype(np.float32)

    def __getitem__(self, idx):
        # ... (ì´ì „ ì½”ë“œ ë™ì¼) ...
        # 1. í˜„ì¬(Target) ìƒ˜í”Œ ì •ë³´ ë° ë¡œë“œ
        row = self.df.iloc[idx]
        subject_id_str = row['Subject ID']
        mri_id = row['MRI ID']
        age = float(row['Age']) / 100.0 
        subject_id_int = int(self.subject_to_id[subject_id_str])
        target_data = self._load_scan_data(mri_id)

        # ---------------- [ìˆ˜ì •ëœ ë¶€ë¶„ ì‹œì‘] ----------------
        # 3. Reference ë°ì´í„° ì°¾ê¸° (2ì¥ í•„ìš”)
        subj_rows = self.df[self.df['Subject ID'] == subject_id_str]
        candidates = subj_rows[subj_rows['MRI ID'] != mri_id] # ë‚˜ ìì‹  ì œì™¸
        
        needed_frames = 2  # í•„ìš”í•œ Reference ìˆ˜
        refs = []

        if len(candidates) >= needed_frames:
            # í›„ë³´ê°€ 2ê°œ ì´ìƒì´ë©´: ëœë¤í•˜ê²Œ 2ê°œ ì„ íƒ (ë¹„ë³µì› ì¶”ì¶œ)
            selected_rows = candidates.sample(n=needed_frames, replace=False)
            for _, r_row in selected_rows.iterrows():
                refs.append(self._load_scan_data(r_row['MRI ID']))
                
        elif len(candidates) > 0:
            # í›„ë³´ê°€ 1ê°œë¿ì´ë©´: ê·¸ 1ê°œë¥¼ ê°€ì ¸ì˜¤ê³ , ë¶€ì¡±í•œ ë§Œí¼ ë³µì œ
            r_row = candidates.iloc[0]
            d = self._load_scan_data(r_row['MRI ID'])
            refs.append(d)
            # ë¶€ì¡±í•œ ë§Œí¼ ë³µì œ (1ê°œ ì¶”ê°€)
            while len(refs) < needed_frames:
                refs.append(d.copy())
                
        else:
            # í›„ë³´ê°€ ì•„ì˜ˆ ì—†ìœ¼ë©´ (ë‚˜ í˜¼ìë©´): Target ë°ì´í„°ë¥¼ ë³µì œí•´ì„œ ì±„ì›€
            d = target_data.copy()
            while len(refs) < needed_frames:
                refs.append(d)

        # (2, C, N) í˜•íƒœë¡œ ìŠ¤íƒ
        ref_data = np.stack(refs, axis=0) 
        # ---------------- [ìˆ˜ì •ëœ ë¶€ë¶„ ë] ----------------

        # 4. ë¦¬í„´: (Target, MRI_ID, Age, ID, Reference)
        return target_data, mri_id, age, subject_id_int, ref_data


def get_args():
    parser = argparse.ArgumentParser()

    # Dataset & dataloader
    parser.add_argument("--sphere", type=str, default="/data/object/sphere/unist/icosphere_6.vtk", help="Sphere mesh (vtk or FreeSurfer format)")
    parser.add_argument("--data-norm", action="store_true", help="Z-score+prctile data normalization")
    parser.add_argument("--preload", type=str, choices=["none", "cpu", "device"], default="device", help="Data preloading")
    parser.add_argument("--in-ch", type=str, default=["curv", "sulc", "inflated.H"], nargs="+", help="List of geometry")
    parser.add_argument("--hemi", type=str, nargs="+", choices=["lh", "rh"], help="Hemisphere for learning", required=True)
    
    parser.add_argument("--csv-file", type=str, default="/data/human/OASIS/OASIS2/oasis_longitudinal_demographics.xlsx", required=True, help="OASIS longitudinal Excel íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--surf-dir", type=str, default="/data/human/OASIS/OASIS2/Freesurfer", required=True, help="Freesurfer í‘œë©´ ë°ì´í„°ê°€ ìˆëŠ” ê¸°ë³¸ ë””ë ‰í† ë¦¬")
    
    parser.add_argument("--classes", type=int, nargs="+", help="List of regions of interest")
    parser.add_argument("--seed", type=int, default=0, help="Random seed for data shuffling")
    parser.add_argument("--aug", type=int, default=0, help="Level of data augmentation")

    parser.add_argument("--test-split-ratio", type=float, default=0.2,help="í…ŒìŠ¤íŠ¸ì…‹ìœ¼ë¡œ ë¶„ë¦¬í•  í”¼í—˜ì ë¹„ìœ¨")
    
    # Training and evaluation
    parser.add_argument("--train-num-steps", type=int, default=20, help="Max epoch")
    parser.add_argument("--batch-size", type=int, default=1, help="Batch size")
    parser.add_argument("--learning-rate", type=float, default=8e-5, help="Initial learning rate")
    parser.add_argument("--no-decay", action="store_true", help="Disable decay (every 2 epochs if no progress)")
    parser.add_argument("--loss", type=str, default="dl", choices=["dl", "ce"], help="dl: Dice loss, ce: cross entropy")
    parser.add_argument("--log-dir", type=str, default="./logs", help="Path to the log files (output)")
    parser.add_argument("--ckpt-dir", type=str, default="./logs", help="Path to the checkpoint file (output)")
    parser.add_argument("--resume", type=str, default=None, help="Checkpoint (pth) to resume training")
    parser.add_argument("--save-and-sample-every", type=int, default=1000)
    parser.add_argument("--num-samples", type=int, default=2)
    parser.add_argument("--results-dir", type=str, default="/data/lfs/pline1207/SPHARM-Diffusion/results")
    parser.add_argument("--data-normalization", action="store_true")
    
    # diffusion settings
    parser.add_argument("--timesteps", type=int, default=1000)
    parser.add_argument("--sampling-timesteps", type=int, default=None)
    parser.add_argument("--objective", type=str, choices=["pred_v", "pred_x0", "pred_noise"], default="pred_v")
    parser.add_argument("--beta-schedule", type=str, choices=["linear", "cosine", "sigmoid"], default="cosine")
    parser.add_argument("--auto-normalize", action="store_true")
    
    # SPHARM-Net settings
    parser.add_argument("-D", "--depth", type=int, default=3, help="Depth of SPHARM-Net")
    parser.add_argument("-C", "--channel", type=int, default=128, help="# of channels in the entry layer of SPHARM-Net")
    parser.add_argument("-L", "--bandwidth", type=int, default=80, help="Bandwidth of SPHARM-Net")
    parser.add_argument("--interval", type=int, default=5, help="Anchor interval of hamonic coefficients")
    
    # ğŸŒŸ [ì¶”ê°€] ViViT (Reference Image) Settings
    parser.add_argument("--use-ref", action="store_true", help="ViViTë¥¼ ì‚¬ìš©í•´ Reference Imageë¥¼ ì¡°ê±´ìœ¼ë¡œ ì¤„ ê²ƒì¸ì§€ ì—¬ë¶€")
    parser.add_argument("--ref-frames", type=int, default=2, help="ViViTì— ë“¤ì–´ê°ˆ Reference Frame ìˆ˜ (ë³´í†µ 2)")

    # Machine settings
    parser.add_argument("--gpu", type=int, default=0, help="GPU ID for training")
    parser.add_argument("--no-cuda", action="store_true", help="No CUDA")
    parser.add_argument("--threads", type=int, default=1, help="# of CPU threads")

    args = parser.parse_args()
    return args


def main(args):
    args.cuda = not args.no_cuda and torch.cuda.is_available()
    device = torch.device(f"cuda:{args.gpu}" if args.cuda else "cpu")
    preload = None if args.preload == "none" else device if args.preload == "device" else args.preload

    torch.set_num_threads(args.threads)
    if not args.cuda:
        torch.set_num_threads(args.threads)

    torch.manual_seed(args.seed)
    if args.cuda:
        torch.cuda.manual_seed(args.seed)

    print("Loading data...")
    sphere = os.path.join(args.sphere)
    v, _ = read_mesh(sphere)

    # í†µí•© ID ë§µ ìƒì„±
    print(f"ì‚¬ì „ ë¡œë“œ (í†µí•© ID ë§µ ìƒì„±ìš©): {args.csv_file}")
    try:
        full_df = pd.read_excel(args.csv_file)
        full_df = full_df.dropna(subset=['Subject ID'])
    except Exception as e:
        raise FileNotFoundError(f"Excel íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {args.csv_file} - {e}")

    all_subjects_sorted = sorted(full_df['Subject ID'].unique())
    total_unique_subjects = len(all_subjects_sorted)
    
    subject_to_id_map = {subj: i for i, subj in enumerate(all_subjects_sorted)}
    
    print(f"ì´ {total_unique_subjects}ëª…ì˜ ê³ ìœ  í”¼í—˜ìì— ëŒ€í•´ í†µí•© ID ë§µ ìƒì„± ì™„ë£Œ.")

    # í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë¶„ë¦¬
    rng = np.random.RandomState(args.seed)
    shuffled_subjects = list(all_subjects_sorted)
    rng.shuffle(shuffled_subjects)
    
    split_idx = int(total_unique_subjects * (1.0 - args.test_split_ratio))
    train_subjects_set = set(shuffled_subjects[:split_idx])
    test_subjects_set = set(shuffled_subjects[split_idx:])
    
    # max_time_obs ê³„ì‚°
    full_df['MR Delay'] = full_df['MR Delay'].apply(lambda x: str(x).replace('M', '').strip())
    full_df['MR Delay'] = pd.to_numeric(full_df['MR Delay'], errors='coerce').fillna(0)
    max_time_obs = full_df['MR Delay'].max()

    # í›ˆë ¨ ë°ì´í„°ì…‹
    ds_train = OASISLongitudinalDataset(
        csv_path=args.csv_file,
        surf_base_dir=args.surf_dir,
        in_ch=args.in_ch,
        hemi=args.hemi,
        subject_to_id_map=subject_to_id_map,
        subject_partition_set=train_subjects_set,
        max_time_obs_overall=max_time_obs,
        data_norm=args.data_norm,
        preload=preload,
        data_normalization=args.data_normalization,
        sphere_path=sphere,
        partition='train'
    )

    # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹
    ds_test = OASISLongitudinalDataset(
        csv_path=args.csv_file,
        surf_base_dir=args.surf_dir,
        in_ch=args.in_ch,
        hemi=args.hemi,
        subject_to_id_map=subject_to_id_map,
        subject_partition_set=test_subjects_set,
        max_time_obs_overall=max_time_obs,
        data_norm=args.data_norm,
        preload="none", 
        data_normalization=args.data_normalization,
        sphere_path=sphere,
        partition='test'
    )

    model_in_channels = len(args.in_ch) * len(args.hemi)
    if model_in_channels == 0:
        raise ValueError("ì…ë ¥ ì±„ë„(--in-ch)ê³¼ ë°˜êµ¬(--hemi)ë¥¼ 1ê°œ ì´ìƒ ì§€ì •í•´ì•¼ í•©ë‹ˆë‹¤.")

    print(f"ëª¨ë¸ì— ì „ë‹¬í•  ì´ ê³ ìœ  í”¼í—˜ì ìˆ˜: {total_unique_subjects}")
    print(f"ViViT Reference ì‚¬ìš© ì—¬ë¶€: {args.use_ref}")
    
    # SPHARM_Net í˜¸ì¶œ
    model = SPHARM_Net(
        sphere=sphere,
        device=device,
        in_ch=model_in_channels,
        n_class=model_in_channels,
        C=args.channel,
        L=args.bandwidth,
        D=args.depth,
        interval=args.interval,
        self_condition=False,
        verbose=False,
        add_xyz=True,
        # Longitudinal + ViViT ê´€ë ¨ ì¸ì ì „ë‹¬
        max_time_obs=max_time_obs,
        num_subjects=total_unique_subjects,
        # ğŸŒŸ [ìˆ˜ì •] ViViT í™œì„±í™”
        use_ref_condition=args.use_ref,     
        ref_in_ch=model_in_channels,        
        ref_num_frames=args.ref_frames      
    )
    model.to(device)

    print("train: auto_normalize: ", args.auto_normalize)
    diffusion = GaussianDiffusion(
        model=model,
        signal_size=v.shape[0],
        timesteps=args.timesteps,
        sampling_timesteps=args.sampling_timesteps,
        objective=args.objective,
        beta_schedule=args.beta_schedule,
        auto_normalize=args.auto_normalize
    )
    diffusion.to(device)
    
    trainer = SphericalTrainer(
        diffusion_model=diffusion,
        dataset=ds_train,
        train_batch_size=args.batch_size,
        gradient_accumulate_every=2,
        train_lr=args.learning_rate,
        train_num_steps=args.train_num_steps,
        ema_decay=0.995,
        results_folder=args.results_dir,
        save_and_sample_every=args.save_and_sample_every,
        num_samples=args.num_samples,
        log_dir=args.log_dir,
        valid_dataset=ds_test 
    )
    trainer.train()


if __name__ == "__main__":
    args = get_args()
    main(args)
